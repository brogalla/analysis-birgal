{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ecb046-2069-4731-912e-2764addcc302",
   "metadata": {},
   "source": [
    "# Bias correct atmospheric conditions\n",
    "\n",
    "- spatially varying field that is constant in time\n",
    "- Calculated from the difference between ERA5 1979-2015 time-mean and PACE ensemble-mean and time-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a8a586-bb06-4b50-8024-eeb8d5815aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cmocean\n",
    "import sys\n",
    "sys.path.append('/home/users/birgal/')\n",
    "import pickle\n",
    "from nemo_python_git.utils import fix_lon_range\n",
    "from nemo_python_git.interpolation import regrid_array_cf, regrid_operator_cf\n",
    "from nemo_python_git.forcing import find_cesm2_file\n",
    "from nemo_python_git.ics_obcs import fill_ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c432f9c-2976-4233-8898-96832d5cffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = '/gws/nopw/j04/anthrofail/birgal/NEMO_AIS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e52b56-2a88-43fd-8d36-59554f4b8609",
   "metadata": {},
   "source": [
    "Start with thermodynamic variables (TREFHT, QREFHT, FLDS, FSDS, PRECT, PRECS, PSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe8f64-2589-4289-8a6c-96945eb0634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atm_bias_correct(source, variable, expt='LE2', year_start=1979, year_end=2015, \n",
    "                     ensemble_mean_file=None, era5_mean_file=None):\n",
    "\n",
    "    # process_forcing_for_correction(source, variable)\n",
    "    if source=='CESM2':\n",
    "        # Read in ensemble time mean (or calculate it)\n",
    "        if ensemble_mean_file:\n",
    "            CESM2_time_mean = xr.open_dataset(ensemble_mean_file)\n",
    "        else:\n",
    "            CESM2_time_mean = cesm2_ensemble_time_mean_forcing(expt, variable, year_start=year_start, year_end=year_end)\n",
    "\n",
    "        # Read in time mean of ERA5 files (or calculate it)\n",
    "        if era5_mean_file:\n",
    "            ERA5_time_mean = xr.open_dataset(era5_mean_file)\n",
    "        else:\n",
    "            ERA5_time_mean = era5_time_mean_forcing(expt, variable, year_start=year_start, year_end=year_end)\n",
    "        \n",
    "        # Interpolate time means to eANT025 grids since they need to be on the same grid to do the correction\n",
    "        \n",
    "        # thermodynamic correction\n",
    "        if variable in ['TREFHT','QREFHT','FLDS','FSDS']:\n",
    "            print('Correcting thermodynamics')\n",
    "            thermo_correction(variable, CESM2_time_mean, ERA5_time_mean, out_file)\n",
    "            \n",
    "        # wind correction\n",
    "        elif variable in ['UBOT','VBOT']:\n",
    "            print('Correcting katabatic winds')\n",
    "            katabatic_correction(variable, CESM2_time_mean, ERA5_time_mean, out_file)\n",
    "        else:\n",
    "            print(f'Variable {variable} does not need bias correction. Check that this is true.')\n",
    "    else:\n",
    "        raise Exception(\"Bias correction currently only set up to correct CESM2, sorry you'll need to write some more code\")\n",
    "\n",
    "    return\n",
    "\n",
    "# Function calculates the time-mean over specified year range for mean of all CESM2 ensemble members in the specified experiment\n",
    "# Input:\n",
    "# - expt : string of CESM2 experiment name (e.g. 'LE2')\n",
    "# - variable : string of forcing variable name\n",
    "# - (optional) year_start : start year for time averaging\n",
    "# - (optional) end_year   : end year for time averaging\n",
    "# - (optinoal) out_file   : path to file to write time mean to NetCDF in case you want to store it\n",
    "def cesm2_ensemble_time_mean_forcing(expt, variable, year_start=1979, year_end=2015, out_file=None):\n",
    "\n",
    "    if expt =='LE2':\n",
    "        ensemble_members = ['1001.001','1011.001','1021.002','1031.002','1041.003','1051.003','1061.004', \\\n",
    "                            '1071.004','1081.005','1091.005','1101.006'] # consider adding this to a central python file to read in\n",
    "\n",
    "    # calculate ensemble mean for each year\n",
    "    year_mean = xr.Dataset()\n",
    "    for year in range(year_start, year_end+1):\n",
    "        files_to_open = []\n",
    "        for ens in ensemble_members:\n",
    "            file_path     = find_cesm2_file(expt, variable, 'atm', '1d', ens, year)\n",
    "            files_to_open = files_to_open.append(file_path)\n",
    "        # calculate ensemble mean    \n",
    "        files    = xr.open_mfdataset(files_to_open, concat_dim='ens', combine='nested')\n",
    "        ens_mean = files[variable].mean(dim='time') # dimensions should be x,y\n",
    "        # save ensemble mean to xarray dataset\n",
    "        if year == year_start:\n",
    "            year_mean = ens_mean\n",
    "        else:\n",
    "            year_mean = xr.concat([year_mean, ens_mean], dim='year')\n",
    "            \n",
    "    # and then calculate time-mean of all ensemble means:\n",
    "    time_mean = year_mean.mean(dim='year')\n",
    "    if out_file:\n",
    "        time_mean.to_netcdf(out_file)\n",
    "    \n",
    "    return time_mean\n",
    "\n",
    "    \n",
    "def time_mean_forcing(source, variable):\n",
    "\n",
    "    # daily forcing files\n",
    "    ERA5_ds = xr.open_mfdataset(f'{base_folder}ERA5-forcing/files/era5_{variable}*') \n",
    "    ERA5_mean = ERA5_ds[variable].mean(dim='time')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def thermo_correction():\n",
    "\n",
    "    # name remapping for variables\n",
    "    ERA5_to_CESM2_varnames = {'TREFHT':'t2m','FSDS':'','FLDS':'','QREFHT':''}\n",
    "                                         \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b54712-1798-4ead-a693-ddb78a5e4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a correction file for a thermodynamic variable, which will add a spatially-varying offset to UKESM/PACE data so that\n",
    "# it matches ERA5 data in the time-mean.\n",
    "def thermo_correction (grid_dir, var_name, cmip_file, era5_file, out_file, prec=64):\n",
    "\n",
    "    grid = Grid(grid_dir)\n",
    "    data = []\n",
    "    for fname in [cmip_file, era5_file]:\n",
    "        data.append(read_netcdf(fname, var_name))\n",
    "    data_diff = data[1] - data[0]\n",
    "    if len(data_diff.shape) == 2:\n",
    "        latlon_plot(data_diff, grid, ctype='plusminus', figsize=(10,6))\n",
    "    else:\n",
    "        titles = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\n",
    "        fig, gs, cax = set_panels('3x4+1C1')\n",
    "        cmap, vmin, vmax = set_colours(data_diff, ctype='plusminus')\n",
    "        for n in range(12+1):\n",
    "            if n == 12:\n",
    "                ax = plt.subplot(gs[0,3])\n",
    "                img = ax.pcolormesh(np.mean(data_diff,axis=0), cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "                ax.set_title('Annual')\n",
    "            else:\n",
    "                ax = plt.subplot(gs[n//4+1, n%4])\n",
    "                img = ax.pcolormesh(data_diff[n,:], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "                ax.set_title(titles[n])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.axis('tight')\n",
    "        plt.colorbar(img, cax=cax, orientation='horizontal')\n",
    "        plt.text(0.05, 0.95, var_name+' correction', transform=fig.transFigure, fontsize=20, ha='left', va='top')\n",
    "        finished_plot(fig, fig_name=var_name+'_correction.png')\n",
    "    write_binary(data_diff, out_file, prec=prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45e338-a5ba-44af-9071-a61bee77f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read forcing (var='wind' or 'thermo') from a given atmospheric dataset (source='ERA5', 'UKESM', or 'PACE'). \n",
    "# Time-average, ensemble-average (if PACE) and interpolate to the MITgcm grid. Save the otuput to a NetCDF file. \n",
    "# This will be used to create spatially-varying, time-constant bias correction files in the functions katabatic_correction\n",
    "# and thermo_correction. Can also set monthly_clim=True to get monthly climatology instead of constant in time.\n",
    "def process_forcing_for_correction (source, mit_grid_dir, out_file, in_dir=None, start_year=1979, end_year=None, monthly_clim=False):\n",
    "\n",
    "    # Set parameters based on source dataset\n",
    "    if source == 'ERA5':\n",
    "        if in_dir is None:\n",
    "            # Path on BAS servers\n",
    "            in_dir = '/data/oceans_input/processed_input_data/ERA5/'\n",
    "        file_head = 'ERA5_'\n",
    "        gtype = ['t', 't', 't', 't', 't']\n",
    "        per_day = 4\n",
    "    elif source == 'PACE':\n",
    "        if in_dir is None:\n",
    "            # Path on BAS servers\n",
    "            in_dir = '/data/oceans_input/processed_input_data/CESM/PACE_new/'\n",
    "        file_head = 'PACE_ens'\n",
    "        num_ens = 20\n",
    "        missing_ens = 13\n",
    "        var_names_in = ['TREFHT', 'QBOT', 'PRECT', 'FSDS', 'FLDS']\n",
    "        monthly = [False, False, False, True, True]\n",
    "        gtype = ['t', 't', 't', 't', 't']\n",
    "    else:\n",
    "        print(('Error (process_forcing_for_correction): invalid source ' + source))\n",
    "        sys.exit()\n",
    "    \n",
    "    # Set parameters based on variable type\n",
    "    var_names = ['atemp', 'aqh', 'precip', 'swdown', 'lwdown']\n",
    "    units = ['degC', '1', 'm/s', 'W/m^2', 'W/m^2']\n",
    "    # Check end_year is defined\n",
    "    if end_year is None:\n",
    "        print('Error (process_forcing_for_correction): must set end_year. Typically use 2014 for WSFRIS and 2013 for PACE.')\n",
    "        sys.exit()\n",
    "\n",
    "    mit_grid_dir = real_dir(mit_grid_dir)\n",
    "    in_dir = real_dir(in_dir)\n",
    "\n",
    "    print('Building grids')\n",
    "    if source == 'ERA5':\n",
    "        forcing_grid = ERA5Grid()\n",
    "    elif source == 'PACE':\n",
    "        forcing_grid = CAMGrid()\n",
    "    mit_grid = Grid(mit_grid_dir)\n",
    "\n",
    "    if monthly_clim:\n",
    "        dim_code = 'xyt'\n",
    "    else:\n",
    "        dim_code = 'xy'\n",
    "    ncfile = NCfile(out_file, mit_grid, dim_code)\n",
    "\n",
    "    # Loop over variables\n",
    "    for n in range(len(var_names)):\n",
    "        print(('Processing variable ' + var_names[n]))\n",
    "        # Read the data, time-integrating as we go\n",
    "        data = None\n",
    "        num_time = 0\n",
    "\n",
    "        if source == 'ERA5':\n",
    "            # Loop over years\n",
    "            for year in range(start_year, end_year+1):\n",
    "                file_path = in_dir + file_head + var_names[n] + '_' + str(year)\n",
    "                data_tmp = read_binary(file_path, [forcing_grid.nx, forcing_grid.ny], 'xyt')\n",
    "                if monthly_clim:\n",
    "                    # Average over each month\n",
    "                    data_sum = np.zeros([12, data_tmp.shape[1], data_tmp.shape[2]])\n",
    "                    t = 0\n",
    "                    for m in range(12):\n",
    "                        nt = days_per_month(m+1, year)*per_day\n",
    "                        data_sum[m,:] = np.mean(data_tmp[t:t+nt,:], axis=0)\n",
    "                        t += nt\n",
    "                    num_time += 1  # in years\n",
    "                else:\n",
    "                    # Integrate over entire year\n",
    "                    data_sum = np.sum(data_tmp, axis=0)\n",
    "                    num_time += data_tmp.shape[0]  # in timesteps\n",
    "                if data is None:\n",
    "                    data = data_sum\n",
    "                else:\n",
    "                    data += data_sum\n",
    "                    \n",
    "        elif source == 'PACE':\n",
    "            # Loop over years\n",
    "            for year in range(start_year, end_year+1):\n",
    "                # Loop over ensemble members\n",
    "                data_tmp = None\n",
    "                num_ens_tmp = 0\n",
    "                for ens in range(1, num_ens+1):\n",
    "                    file_path = in_dir + file_head + str(ens).zfill(2) + '_' + var_names_in[n] + '_' + str(year)\n",
    "                    data_tmp_ens = read_binary(file_path, [forcing_grid.nx, forcing_grid.ny], 'xyt')\n",
    "                    if data_tmp is None:\n",
    "                        data_tmp = data_tmp_ens\n",
    "                    else:\n",
    "                        data_tmp += data_tmp_ens\n",
    "                    num_ens_tmp += 1\n",
    "                # Ensemble mean for this year\n",
    "                data_tmp /= num_ens_tmp\n",
    "                # Now accumulate time integral                    \n",
    "                if monthly_clim:\n",
    "                    data_sum = np.zeros([12, data_tmp.shape[1], data_tmp.shape[2]])\n",
    "                    t = 0\n",
    "                    for m in range(12):\n",
    "                        if monthly[n]:\n",
    "                            # Already have monthly averages\n",
    "                            data_sum[m,:] = data_tmp[m,:]\n",
    "                        else:\n",
    "                            ndays = days_per_month(m+1, year, allow_leap=False)\n",
    "                            data_sum[m,:] = np.mean(data_tmp[t:t+ndays,:], axis=0)\n",
    "                            t += ndays\n",
    "                    num_time += 1\n",
    "                else:\n",
    "                    if monthly[n]:\n",
    "                        # Have to weight monthly averages\n",
    "                        for m in range(12):\n",
    "                            ndays = days_per_month(m+1, year, allow_leap=False)\n",
    "                            data_tmp[m,:] *= ndays\n",
    "                            num_time += ndays\n",
    "                    else:\n",
    "                        num_time += data_tmp.shape[0]\n",
    "                    data_sum = np.sum(data_tmp, axis=0)\n",
    "                if data is None:\n",
    "                    data = data_sum\n",
    "                else:\n",
    "                    data += data_sum\n",
    "\n",
    "        # Now convert from time-integral to time-average\n",
    "        data /= num_time\n",
    "\n",
    "        forcing_lon, forcing_lat = forcing_grid.get_lon_lat(gtype=gtype[n], dim=1)\n",
    "        # Get longitude in the range -180 to 180, then split and rearrange so it's monotonically increasing        \n",
    "        forcing_lon = fix_lon_range(forcing_lon)\n",
    "        i_split = np.nonzero(forcing_lon < 0)[0][0]\n",
    "        forcing_lon = split_longitude(forcing_lon, i_split)\n",
    "        data = split_longitude(data, i_split)\n",
    "        # Now interpolate to MITgcm tracer grid        \n",
    "        mit_lon, mit_lat = mit_grid.get_lon_lat(gtype='t', dim=1)\n",
    "        print('Interpolating')\n",
    "        if monthly_clim:\n",
    "            data_interp = np.empty([12, mit_grid.ny, mit_grid.nx])\n",
    "            for m in range(12):\n",
    "                print(('...month ' + str(m+1)))\n",
    "                data_interp[m,:] = interp_reg_xy(forcing_lon, forcing_lat, data[m,:], mit_lon, mit_lat)\n",
    "        else:\n",
    "            data_interp = interp_reg_xy(forcing_lon, forcing_lat, data, mit_lon, mit_lat)\n",
    "        print(('Saving to ' + out_file))\n",
    "        ncfile.add_variable(var_names[n], data_interp, dim_code, units=units[n])\n",
    "\n",
    "    ncfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185341a-f612-4dca-b86d-4d802a80f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build katabatic correction files which scale and rotate the winds in a band around the coast. The arguments cmip_file and\n",
    "# era5_file are the outputs of process_forcing_for_correction, for UKESM/PACE and ERA5 respectively.\n",
    "# Update 13 March 2020: Can set bounds on region in domain to apply this correction to. For example, in PAS \n",
    "# can set xmin=-90 to only correct in the eastern part of the domain. \n",
    "def katabatic_correction (grid_dir, cmip_file, era5_file, out_file_scale, out_file_rotate, scale_dist=150., scale_cap=3, xmin=None, xmax=None, ymin=None, ymax=None, prec=64):\n",
    "\n",
    "    var_names = ['uwind', 'vwind']\n",
    "    # Radius for smoothing\n",
    "    sigma = 2\n",
    "\n",
    "    print('Building grid')\n",
    "    grid = Grid(grid_dir)\n",
    "    print('Selecting coastal points')\n",
    "    coast_mask = grid.get_coast_mask(ignore_iceberg=True)\n",
    "    lon_coast = grid.lon_2d[coast_mask].ravel()\n",
    "    lat_coast = grid.lat_2d[coast_mask].ravel()\n",
    "    if xmin is None:\n",
    "        xmin = np.amin(grid.lon_2d)\n",
    "    if xmax is None:\n",
    "        xmax = np.amax(grid.lon_2d)\n",
    "    if ymin is None:\n",
    "        ymin = np.amin(grid.lat_2d)\n",
    "    if ymax is None:\n",
    "        ymax = np.amax(grid.lat_2d)\n",
    "\n",
    "    print('Calculating winds in polar coordinates')\n",
    "    magnitudes = []\n",
    "    angles = []\n",
    "    for fname in [cmip_file, era5_file]:\n",
    "        u = read_netcdf(fname, var_names[0])\n",
    "        v = read_netcdf(fname, var_names[1])\n",
    "        magnitudes.append(np.sqrt(u**2 + v**2))\n",
    "        angle = np.arctan2(v,u)\n",
    "        angles.append(angle)\n",
    "\n",
    "    print('Calculating corrections')\n",
    "    # Take minimum of the ratio of ERA5 to CMIP wind magnitude, and the scale cap\n",
    "    scale = np.minimum(magnitudes[1]/magnitudes[0], scale_cap)\n",
    "    # Smooth and mask the land and ice shelf\n",
    "    scale = mask_land_ice(smooth_xy(scale, sigma=sigma), grid)\n",
    "    # Take difference in angles\n",
    "    rotate = angles[1] - angles[0]\n",
    "    # Take mod 2pi when necessary\n",
    "    index = rotate < -np.pi\n",
    "    rotate[index] += 2*np.pi\n",
    "    index = rotate > np.pi\n",
    "    rotate[index] -= 2*np.pi\n",
    "    # Smoothing would be weird with the periodic angle, so just mask\n",
    "    rotate = mask_land_ice(rotate, grid)\n",
    "\n",
    "    print('Calculating distance from the coast')\n",
    "    min_dist = None\n",
    "    # Loop over all the coastal points\n",
    "    for i in range(lon_coast.size):\n",
    "        # Skip over any points that are out of bounds\n",
    "        if lon_coast[i] < xmin or lon_coast[i] > xmax or lat_coast[i] < ymin or lat_coast[i] > ymax:\n",
    "            continue\n",
    "        # Calculate distance of every point in the model grid to this specific coastal point, in km\n",
    "        dist_to_pt = dist_btw_points([lon_coast[i], lat_coast[i]], [grid.lon_2d, grid.lat_2d])*1e-3\n",
    "        if min_dist is None:\n",
    "            # Initialise the array\n",
    "            min_dist = dist_to_pt\n",
    "        else:\n",
    "            # Figure out which cells have this coastal point as the closest one yet, and update the array\n",
    "            index = dist_to_pt < min_dist\n",
    "            min_dist[index] = dist_to_pt[index]\n",
    "\n",
    "    print('Tapering function offshore')\n",
    "    # Cosine function moving from scaling factor to 1 over distance of scale_dist km offshore\n",
    "    scale_tapered = (min_dist < scale_dist)*(scale - 1)*np.cos(np.pi/2*min_dist/scale_dist) + 1\n",
    "    # For the rotation, move from scaling factor to 0\n",
    "    rotate_tapered = (min_dist < scale_dist)*rotate*np.cos(np.pi/2*min_dist/scale_dist)    \n",
    "\n",
    "    print('Plotting')\n",
    "    data_to_plot = [min_dist, scale_tapered, rotate_tapered]\n",
    "    titles = ['Distance to coast (km)', 'Scaling factor', 'Rotation factor']\n",
    "    ctype = ['basic', 'ratio', 'plusminus']\n",
    "    fig_names = ['min_dist.png', 'scale.png', 'rotate.png']\n",
    "    for i in range(len(data_to_plot)):\n",
    "        for fig_name in [None, fig_names[i]]:\n",
    "            latlon_plot(data_to_plot[i], grid, ctype=ctype[i], include_shelf=False, title=titles[i], figsize=(10,6), fig_name=fig_name)\n",
    "\n",
    "    print('Writing to file')\n",
    "    fields = [scale_tapered, rotate_tapered]\n",
    "    out_files = [out_file_scale, out_file_rotate]\n",
    "    for n in range(len(fields)):\n",
    "        # Replace mask with zeros\n",
    "        mask = fields[n].mask\n",
    "        data = fields[n].data\n",
    "        data[mask] = 0\n",
    "        write_binary(data, out_files[n], prec=prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6034c-a970-41a8-a07f-0609a070e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a correction file for a thermodynamic variable, which will add a spatially-varying offset to UKESM/PACE data so that\n",
    "# it matches ERA5 data in the time-mean.\n",
    "def thermo_correction (grid_dir, var_name, cmip_file, era5_file, out_file, prec=64):\n",
    "\n",
    "    grid = Grid(grid_dir)\n",
    "    data = []\n",
    "    for fname in [cmip_file, era5_file]:\n",
    "        data.append(read_netcdf(fname, var_name))\n",
    "    data_diff = data[1] - data[0]\n",
    "    if len(data_diff.shape) == 2:\n",
    "        latlon_plot(data_diff, grid, ctype='plusminus', figsize=(10,6))\n",
    "    else:\n",
    "        titles = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\n",
    "        fig, gs, cax = set_panels('3x4+1C1')\n",
    "        cmap, vmin, vmax = set_colours(data_diff, ctype='plusminus')\n",
    "        for n in range(12+1):\n",
    "            if n == 12:\n",
    "                ax = plt.subplot(gs[0,3])\n",
    "                img = ax.pcolormesh(np.mean(data_diff,axis=0), cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "                ax.set_title('Annual')\n",
    "            else:\n",
    "                ax = plt.subplot(gs[n//4+1, n%4])\n",
    "                img = ax.pcolormesh(data_diff[n,:], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "                ax.set_title(titles[n])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.axis('tight')\n",
    "        plt.colorbar(img, cax=cax, orientation='horizontal')\n",
    "        plt.text(0.05, 0.95, var_name+' correction', transform=fig.transFigure, fontsize=20, ha='left', va='top')\n",
    "        finished_plot(fig, fig_name=var_name+'_correction.png')\n",
    "    write_binary(data_diff, out_file, prec=prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e762adc-270a-4dce-8c44-5ce3e0fe4dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-env",
   "language": "python",
   "name": "cf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
