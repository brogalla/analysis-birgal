{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039858f1-96ca-40a7-a9c8-d62d25eb0071",
   "metadata": {},
   "source": [
    "# Evaluate water mass properties on the Amundsen Shelf using Pierre's observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b2877-aeb7-47f2-987f-bbe7d91c1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "import cmocean\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/home/users/birgal/')\n",
    "from nemo_python_git.constants import region_bounds, region_names\n",
    "from nemo_python_git.file_io   import read_dutrieux\n",
    "from nemo_python_git.utils     import convert_to_teos10\n",
    "from nemo_python_git.plots     import finished_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ecfd70e-13ff-44a8-8c15-7b879eef70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/gws/nopw/j04/anthrofail/birgal/NEMO_AIS/'\n",
    "run_folder    = f'{base_dir}output/ERA5_test4_cont/'\n",
    "nemo_mesh     = f'{base_dir}bathymetry/mesh_mask-20240305.nc'\n",
    "nemo_domain   = f'{base_dir}bathymetry/domain_cfg-20240305.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b33866f-cd80-4f47-84c4-c1281322b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_mesh_ds  = xr.open_dataset(nemo_mesh).isel(time_counter=0)\n",
    "nemo_mesh_sub = nemo_mesh_ds.isel(x=slice(580, 790), y=slice(200,300))\n",
    "\n",
    "bathy = xr.where((nemo_mesh_sub.isfdraft > 0) | (nemo_mesh_sub.bathy_metry==0), np.nan, nemo_mesh_sub.bathy_metry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f9d40-0ee5-4b15-87ce-fc028e89f7df",
   "metadata": {},
   "source": [
    "### Look at variability of profiles in different regions of the Amundsen Sea\n",
    "\n",
    "regions:\n",
    "- PITW trough\n",
    "- Dotson front\n",
    "- Pine Island Bay\n",
    "- Shelfbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09460e9-ad27-4ec4-afed-86b40cd9c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridT_files  = glob.glob(f'{run_folder}/*grid_T*')\n",
    "# nemo_ds      = xr.open_dataset(gridT_files[-10]).isel(time_counter=0) # load all thegridT files in the run folder\n",
    "nemo_ds      = xr.open_mfdataset(gridT_files, engine='netcdf4', chunks={'x_grid_T':400, 'y_grid_T':200, 'deptht':10})\n",
    "nemo_ds      = nemo_ds.rename({'x_grid_T':'x', 'y_grid_T':'y', 'nav_lon_grid_T':'nav_lon', 'nav_lat_grid_T':'nav_lat', 'deptht':'depth'}) \n",
    "# read in simulation dataset and average by year, then create profiles for each year\n",
    "\n",
    "# load observations:\n",
    "obs          = read_dutrieux(eos='teos10')\n",
    "dutrieux_obs = obs.assign({'nav_lon':obs.lon, 'nav_lat':obs.lat}).rename_dims({'lat':'y', 'lon':'x'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68592cab-f9b7-4300-ab73-a24cb120bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['amundsen_west_shelf_break', 'pine_island_bay', 'dotson_bay']\n",
    "region_zlim = [1500, 1000, 100]\n",
    "r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6644a9c-7570-4bb5-9cd3-c10a015516fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_sims_profile(region, variable, nemo_results, nemo_mesh_ds):\n",
    "    \n",
    "    [xmin, xmax, ymin, ymax] = region_bounds[region]\n",
    "    mask_region      = (nemo_results.nav_lon >= xmin)*(nemo_results.nav_lon <= xmax)*(nemo_results.nav_lat >= ymin)*(nemo_results.nav_lat <= ymax)\n",
    "    nemo_land_mask   = nemo_mesh_ds.tmask.rename({'nav_lev':'depth'})\n",
    "    \n",
    "    variable_region  = xr.where(mask_region*(nemo_land_mask==1), nemo_results[variable], np.nan)\n",
    "    dA_region        = xr.where(mask_region*(nemo_land_mask==1), nemo_results.area_grid_T, np.nan)\n",
    "\n",
    "    variable_profile = (variable_region*dA_region).sum(dim=['x','y'])/dA_region.sum(dim=['x','y'])\n",
    "    \n",
    "    return variable_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20b82c-ae6e-4f32-8142-e2df8fd0cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/birgal/.conda/envs/jaspy3.10/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/home/users/birgal/.conda/envs/jaspy3.10/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n",
      "/home/users/birgal/.conda/envs/jaspy3.10/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(12,5),  gridspec_kw={'width_ratios':[2,1,2,1]}, dpi=100)\n",
    "fig.suptitle(region_names[regions[r]])\n",
    "\n",
    "ax[0].set_ylabel('Depth (m)')\n",
    "titles = ['Temperature', 'Std', 'Salinity', 'Std']\n",
    "for n, axis in enumerate(ax.ravel()):\n",
    "    ax[n].set_title(titles[n])\n",
    "    ax[n].set_ylim(region_zlim[r], 0)\n",
    "    if n!=0: ax[n].yaxis.set_ticklabels([])\n",
    "        \n",
    "for year in range(2000, 2001):\n",
    "    nemo_results = nemo_ds.isel(time_counter=(nemo_ds.time_counter.dt.year==year))\n",
    "    sims_T = region_sims_profile(regions[r], 'thetao', nemo_results, nemo_mesh_ds)\n",
    "    sims_S = region_sims_profile(regions[r], 'so', nemo_results, nemo_mesh_ds)\n",
    "\n",
    "    ax[0].plot(sims_T, sims_T.depth, '-', color='gray')\n",
    "    ax[2].plot(sims_S, sims_S.depth, '-', color='gray')\n",
    "\n",
    "nemo_results = nemo_ds.isel(time_counter=nemo_ds.time_counter.dt.year>=2000).mean(dim='time_counter')\n",
    "sims_T_mean = region_sims_profile(regions[r], 'thetao', nemo_results, nemo_mesh_ds)\n",
    "sims_S_mean = region_sims_profile(regions[r], 'so', nemo_results, nemo_mesh_ds)\n",
    "ax[0].plot(sims_T_mean, sims_T_mean.depth, '-', color='k')\n",
    "ax[2].plot(sims_S_mean, sims_S_mean.depth, '-', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8e895bb-bc1e-44cf-8902-31df07b0331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mask which is 1 only within these bounds where there is data, and excluding cavities\n",
    "[xmin, xmax, ymin, ymax] = region_bounds[regions[r]]\n",
    "obs_mask_region          = (dutrieux_obs.nav_lon >= xmin)*(dutrieux_obs.nav_lon <= xmax)*(dutrieux_obs.nav_lat >= ymin)*(dutrieux_obs.nav_lat <= ymax)\n",
    "obs_region               = xr.where(obs_mask_region , dutrieux_obs.ConsTemp, np.nan)\n",
    "# obs_profile  = np.sum(obs_region*obs_dA, axis=(1,2))/np.sum(obs_dA, axis=(1,2))\n",
    "\n",
    "# # Calculate time-mean and standard deviation from each source\n",
    "# model_mean      = np.mean(model_data, axis=-2)\n",
    "# model_std       = np.std(model_data, axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933eb80-2874-458a-ba3d-204d6905aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 3x2 plot showing temperature and salinity casts in 3 regions, comparing ERA5-forced output to Pierre's climatology.\n",
    "def plot_ts_casts_obs (obs_dir='/data/oceans_input/processed_input_data/pierre_climatology/', base_dir='./', fig_dir='./'):\n",
    "\n",
    "    obs_dir = real_dir(obs_dir)\n",
    "    base_dir = real_dir(base_dir)\n",
    "    fig_dir = real_dir(fig_dir)\n",
    "    model_dir = base_dir + 'PAS_ERA5/output/'\n",
    "    grid_path = base_dir + 'PAS_grid/'\n",
    "    grid = Grid(grid_path)\n",
    "    obs_file_head = obs_dir + 'ASEctd_griddedMean'\n",
    "    obs_file_tail = '.mat'\n",
    "    obs_years = np.array(years_with_obs(obs_dir))\n",
    "    obs_num_years = len(obs_years)\n",
    "    regions = ['amundsen_west_shelf_break', 'pine_island_bay', 'dotson_bay']\n",
    "    region_titles = [r'$\\bf{a}$. PITW Trough', r'$\\bf{b}$. Pine Island Bay', r'$\\bf{c}$. Dotson front']\n",
    "    num_regions = len(regions)\n",
    "    model_var = ['THETA', 'SALT']\n",
    "    obs_var = ['PTmean', 'Smean']\n",
    "    archive_var = ['temp', 'salt']\n",
    "    var_titles = ['Temperature', 'Salinity']\n",
    "    var_units = [deg_string+'C', 'psu']\n",
    "    num_var = len(model_var)\n",
    "    model_year0 = 1947; \n",
    "    model_start_year = 1979\n",
    "    model_end_year = 2019\n",
    "    model_split_year = 2013\n",
    "    model_years1 = obs_years[obs_years < model_split_year]\n",
    "    model_num_years1 = model_years1.size\n",
    "    model_years2 = obs_years[obs_years >= model_split_year]\n",
    "    model_num_years2 = model_years2.size\n",
    "    obs_smooth = 51\n",
    "    obs_smooth_below = -100\n",
    "\n",
    "    # Read observations and model in one go\n",
    "    print('Reading observations')\n",
    "    obs_data = None; model_data = None;\n",
    "    model_data1 = None; model_data2 = None;\n",
    "    for t in range(obs_num_years):\n",
    "        print(('...'+str(obs_years[t])))\n",
    "        f = loadmat(obs_file_head+str(obs_years[t])+obs_file_tail)\n",
    "        if obs_data is None:\n",
    "            # This is the first year: read the grid and set up arrays\n",
    "            obs_lon, obs_lat, obs_depth, obs_dA, obs_dV = pierre_obs_grid(f, xy_dim=2, z_dim=1, dA_dim=3)\n",
    "            # Get MITgcm's ice mask on this grid\n",
    "            obs_ice_mask = interp_reg_xy(grid.lon_1d, grid.lat_1d, grid.ice_mask.astype(float), obs_lon, obs_lat)\n",
    "            obs_ice_mask[obs_ice_mask < 0.5] = 0\n",
    "            obs_ice_mask[obs_ice_mask >= 0.5] = 1\n",
    "            obs_ice_mask = obs_ice_mask.astype(bool)\n",
    "            obs_data = np.ma.empty([num_regions, num_var, obs_num_years, obs_depth.size])\n",
    "            model_data = np.ma.empty([num_regions, num_var, obs_num_years, obs_depth.size])\n",
    "            model_data1 = np.ma.empty([num_regions, num_var, model_num_years1, obs_depth.size])\n",
    "            model_data2 = np.ma.empty([num_regions, num_var, model_num_years2, obs_depth.size])\n",
    "        for v in range(num_var):\n",
    "            # Read 3D temp or salinity\n",
    "            obs_var_3d = np.transpose(f[obs_var[v]])\n",
    "            obs_var_3d = np.ma.masked_where(np.isnan(obs_var_3d), obs_var_3d)\n",
    "            # Now read model data for Jan-Feb\n",
    "            model_var_3d = read_netcdf(model_dir+str(obs_years[t])+'01/MITgcm/output.nc', model_var[v])[:2,:]\n",
    "            # Time-average, weighting with days per month\n",
    "            ndays = np.array([days_per_month(m+1, obs_years[t]) for m in range(2)])\n",
    "            model_var_3d = np.sum(model_var_3d*ndays[:,None,None,None], axis=0)/np.sum(ndays)\n",
    "            # Fill the land mask with nearest neighbours to not screw up the interpolation\n",
    "            discard = grid.hfac==0\n",
    "            sum_of_regions = np.zeros(grid.hfac.shape)\n",
    "            for r in range(num_regions):\n",
    "                sum_of_regions += grid.get_region_mask(regions[r])\n",
    "            fill = (grid.hfac == 0)*(sum_of_regions > 0)\n",
    "            model_var_3d = discard_and_fill(model_var_3d, discard, fill, log=False)\n",
    "            model_var_3d = np.ma.masked_where(model_var_3d==-9999, model_var_3d)\n",
    "            # Interpolate to observational grid\n",
    "            model_var_3d_interp = interp_reg_xyz(grid.lon_1d, grid.lat_1d, grid.z, model_var_3d, obs_lon, obs_lat, obs_depth)\n",
    "            model_var_3d_interp = np.ma.masked_where(model_var_3d_interp==-9999, model_var_3d_interp)\n",
    "            for r in range(num_regions):\n",
    "                # Area-average over the given region\n",
    "                [xmin, xmax, ymin, ymax] = region_bounds[regions[r]]\n",
    "                # Make a mask which is 1 only within these bounds where there is data, and excluding cavities\n",
    "                mask = (obs_lon >= xmin)*(obs_lon <= xmax)*(obs_lat >= ymin)*(obs_lat <= ymax)*np.invert(obs_ice_mask)\n",
    "                mask = xy_to_xyz(mask, [obs_lat.size, obs_lon.size, obs_depth.size]).astype(float)\n",
    "                mask[obs_var_3d.mask] = 0\n",
    "                obs_profile = np.sum(obs_var_3d*obs_dA*mask, axis=(1,2))/np.sum(obs_dA*mask, axis=(1,2))\n",
    "                model_profile = np.sum(model_var_3d_interp*obs_dA*mask, axis=(1,2))/np.sum(obs_dA*mask, axis=(1,2))\n",
    "                # Make a smoothed version and overwrite with it below 100m depth\n",
    "                obs_profile_smoothed = moving_average(obs_profile, obs_smooth, keep_edges=True)\n",
    "                model_profile_smoothed = moving_average(model_profile, obs_smooth, keep_edges=True)\n",
    "                index = obs_depth < obs_smooth_below\n",
    "                obs_profile[index] = obs_profile_smoothed[index]\n",
    "                model_profile[index] = model_profile_smoothed[index]                \n",
    "                obs_data[r,v,t,:] = obs_profile\n",
    "                model_data[r,v,t,:] = model_profile\n",
    "                if obs_years[t] < model_split_year:\n",
    "                    model_data1[r,v,t,:] = model_profile\n",
    "                else:\n",
    "                    model_data2[r,v,t-model_num_years1,:] = model_profile\n",
    "\n",
    "    # Calculate time-mean and standard deviation from each source\n",
    "    model_mean = np.mean(model_data, axis=-2)\n",
    "    model_mean_excl = np.mean(model_data1, axis=-2)\n",
    "    obs_mean = np.mean(obs_data, axis=-2)\n",
    "    model_std = np.std(model_data, axis=-2)\n",
    "    model_std_excl = np.std(model_data1, axis=-2)\n",
    "    obs_std = np.std(obs_data, axis=-2)\n",
    "    # Also make depth positive\n",
    "    obs_depth *= -1\n",
    "\n",
    "    # Write model profiles to NetCDF file for archiving\n",
    "    z_grid = ZGrid(-obs_depth)\n",
    "    ncfile = NCfile('ts_casts.nc', z_grid, 'zt')\n",
    "    ncfile.add_time(np.array([datetime.datetime(y,2,1) for y in obs_years]))\n",
    "    for r in range(num_regions):\n",
    "        for v in range(num_var):\n",
    "            ncfile.add_variable(regions[r]+'_'+archive_var[v], model_data[r,v,:], 'zt')\n",
    "    ncfile.close()\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(7,12))\n",
    "    gs = plt.GridSpec(3,25)\n",
    "    gs.update(left=0.1, right=0.98, bottom=0.13, top=0.93, wspace=0.2, hspace=0.4)\n",
    "    for r in range(num_regions):\n",
    "        for v in range(num_var):\n",
    "            # Choose first 8 panels and merge them (leaving 1 empty between variables)\n",
    "            ax = plt.subplot(gs[r,v*13:v*13+8])\n",
    "            ax.tick_params(direction='in')\n",
    "            ax.grid(linestyle='dotted')\n",
    "            # Plot each year of observations in thin grey\n",
    "            for t in range(obs_num_years):\n",
    "                ax.plot(obs_data[r,v,t,:], obs_depth, color='DimGrey', linewidth=(1.5 if r==2 and obs_years[t]==2000 else 0.5), label=('Observations (each year)' if t==0 else None))\n",
    "                # Plot each year of model output in thin light blue or red\n",
    "            for t in range(model_num_years1):\n",
    "                ax.plot(model_data1[r,v,t,:], obs_depth, color='DodgerBlue', linewidth=0.5, label=('Model (each year pre-'+str(model_split_year)+')' if t==0 else None))\n",
    "            for t in range(model_num_years2):\n",
    "                ax.plot(model_data2[r,v,t,:], obs_depth, color='LightCoral', linewidth=0.5, label=('Model (each year post-'+str(model_split_year)+')' if t==0 else None))\n",
    "            # Plot observational mean in thick black\n",
    "            ax.plot(obs_mean[r,v,:], obs_depth, color='black', linewidth=1.5, label='Observations (mean/std)')\n",
    "            # Plot model mean in thick blue or red\n",
    "            ax.plot(model_mean[r,v,:], obs_depth, color='blue', linewidth=1.5, label='Model (mean/std, all years)', zorder=2*obs_num_years+2)\n",
    "            ax.plot(model_mean_excl[r,v,:], obs_depth, color='blue', linestyle='dotted', linewidth=1.5, label='Model (mean/std pre-'+str(model_split_year)+')', zorder=2*obs_num_years+1)\n",
    "            # Find the deepest unmasked depth where there is data from both model and obs\n",
    "            y_deep = min(np.amax(np.ma.masked_where(obs_mean[r,v,:].mask, obs_depth)), np.amax(np.ma.masked_where(model_mean[r,v,:].mask, obs_depth)))\n",
    "            if r > 0:\n",
    "                # Manually set to 1km\n",
    "                y_deep = 1000\n",
    "            ax.set_ylim([y_deep,0])\n",
    "            if v==0 and r==0:\n",
    "                plt.ylabel('Depth (m)', fontsize=12)\n",
    "            if v==1:\n",
    "                ax.set_yticklabels([])\n",
    "            if r == num_regions-1:\n",
    "                plt.xlabel(var_units[v], fontsize=12)\n",
    "            plt.title(var_titles[v], fontsize=14)\n",
    "            if v==1 and r==2:\n",
    "                plt.legend(loc='lower center', bbox_to_anchor=(-0.1, -0.62), ncol=2, fontsize=12)\n",
    "            if v==0 and r==2:\n",
    "                # Remove the last tick label so it doesn't get too close\n",
    "                label = ax.get_xticklabels()[-1]\n",
    "                label.set_visible(False)\n",
    "            # Now plot standard deviations\n",
    "            # Choose next 4 panels and merge them\n",
    "            ax2 = plt.subplot(gs[r,v*13+8:v*13+12])\n",
    "            ax2.tick_params(direction='in')\n",
    "            ax2.grid(linestyle='dotted')\n",
    "            ax2.plot(obs_std[r,v,:], obs_depth, color='black', linewidth=1.5)\n",
    "            ax2.plot(model_std[r,v,:], obs_depth, color='blue', linewidth=1.5)\n",
    "            ax2.plot(model_std_excl[r,v,:], obs_depth, color='blue', linestyle='dotted', linewidth=1.5)\n",
    "            ax2.set_yticklabels([])\n",
    "            ax2.set_ylim([y_deep,0])\n",
    "            # Overwrite the labels so there are no unnecessary decimals - otherwise you get an overlap of labels at 0\n",
    "            xticks = ax2.get_xticks()\n",
    "            ax2.set_xticklabels([round_to_decimals(tick,1) for tick in xticks])\n",
    "            plt.title('std', fontsize=12)        \n",
    "        plt.text(0.5, 0.985-0.3*r, region_titles[r], ha='center', va='top', fontsize=18, transform=fig.transFigure)\n",
    "    finished_plot(fig, fig_name=fig_dir+'ts_casts_obs.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9871c-e153-4340-9234-c0050b4b46a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaspy3.10",
   "language": "python",
   "name": "jaspy3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
